{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce3274a",
      "metadata": {
        "id": "4ce3274a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import optuna\n",
        "import joblib\n",
        "import io\n",
        "import pickle as pkl\n",
        "from optuna.samplers import TPESampler\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import Sequential\n",
        "from keras import layers\n",
        "from keras import utils, callbacks\n",
        "from keras.models import Model\n",
        "from keras.layers import LeakyReLU as lrelu\n",
        "from keras.layers import ReLU as relu\n",
        "from keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, Input, MaxPool2D, Permute, GlobalAveragePooling2D, MaxPooling2D\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.backend import clear_session\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.metrics import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqc0AB2j5-Mp",
      "metadata": {
        "id": "cqc0AB2j5-Mp"
      },
      "outputs": [],
      "source": [
        "# #webcam dataset (grassknot.zip)\n",
        "# !gdown 1QQozu0ldXuSVNPKjxRVYE6fCjIgdktwn\n",
        "\n",
        "\n",
        "# # from zipfile import ZipFile\n",
        "# with ZipFile(\"grassknot.zip\",\"r\") as zipObj:\n",
        "#     zipObj.extractall()\n",
        "\n",
        "#fixed hyperparameters\n",
        "SPLIT_RATIO = (0.75,0.15,0.10)\n",
        "IMG_TARGET_SIZE = (64,64)\n",
        "EPOCHS = 30\n",
        "NUM_CLASSES = 26\n",
        "\n",
        "#tuning\n",
        "#data_batch_size = 32,64,128\n",
        "#cnn_layers = 3,4,5,6,7,8\n",
        "#learning_rate = between 0.01 and 0.00001\n",
        "#activation_fn = 'relu, leaky relu, relu6'\n",
        "\n",
        "# #split dataset into train, val, test\n",
        "# import splitfolders\n",
        "# splitfolders.ratio(\"./grassknot/\", output=\"./data/\",\n",
        "#     seed=42, ratio=SPLIT_RATIO, group_prefix=None, move=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401d45d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_results(history,trial_no):\n",
        "  \n",
        "  fig_path = './project/results/hyperparameter_optimization/trial_plots/'+'trial'+str(trial_no)+'.jpg'\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(history.history['accuracy'], label='accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([0, 1])\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Accuracy Curve')\n",
        "\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Loss Curve')\n",
        "  \n",
        "  plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "  plt.tight_layout()\n",
        "  plt.rcParams['savefig.orientation'] = 'landscape'\n",
        "  plt.savefig(fig_path)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09bc04e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_results(test,y_vals):\n",
        "    classes = list(test.class_indices.keys())\n",
        "    cm = confusion_matrix(test.classes,y_vals)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels = classes)\n",
        "    plt.rcParams[\"figure.figsize\"] = (15,10)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./project/results/hyperparameter_optimization/trial_results/'+'confusion_matrix.jpg')\n",
        "    plt.show()\n",
        "    print(classification_report(test.classes, y_vals, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b207eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_distribution(train,val):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87ddb39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "d87ddb39",
        "outputId": "46b99e6a-fcf3-44f4-a3df-049fe66c767a"
      },
      "outputs": [],
      "source": [
        "def hpo(trial):\n",
        "    \n",
        "    clear_session()\n",
        "    \n",
        "    global train,val,test,pred_vals,history,model\n",
        "    \n",
        "    gen_aug = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=15) \n",
        "    gen = ImageDataGenerator(rescale=1./255) \n",
        "    trial_no = trial.number\n",
        "    \n",
        "    #defining the hyperparameters that need tuning\n",
        "    batch_size = trial.suggest_int('batch_size',64,128)\n",
        "    num_layers = trial.suggest_categorical('num_layers', [3,4,5,6])\n",
        "    activation = trial.suggest_categorical('activation',['relu','selu','elu'])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\",1e-5,1e-2,log=True)\n",
        "    \n",
        "    ##image directories - loading data\n",
        "    train = gen_aug.flow_from_directory(\"./data/train\",\n",
        "                                target_size=IMG_TARGET_SIZE,\n",
        "                                batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val = gen.flow_from_directory(\"./data/val\",\n",
        "                              target_size=IMG_TARGET_SIZE,\n",
        "                              batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    test = gen.flow_from_directory(\"./data/test\", \n",
        "                               target_size=IMG_TARGET_SIZE, \n",
        "                               class_mode=None, shuffle=False)\n",
        "    \n",
        "    check_distribution(train,val)\n",
        "    \n",
        "    model_name = ('model_'+str(trial_no))\n",
        "    ##creating the model architecture\n",
        "    model = Sequential(name=model_name)\n",
        "    num_filters = [64, 128, 128, 128, 256, 256, 512, 512]\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        model.add(Conv2D(filters = num_filters[layer], kernel_size = 5, padding = 'same', activation = activation))\n",
        "        if layer != num_layers:\n",
        "            model.add(MaxPooling2D())\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "    \n",
        "    #compile the model\n",
        "    model.compile(loss=CategoricalCrossentropy(),\n",
        "                  optimizer=Adam(learning_rate=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    #callback functions\n",
        "    early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
        "                                        patience=5, restore_best_weights = True)\n",
        "    \n",
        "    #train the model and save it\n",
        "    history = model.fit(x=train, validation_data = val, epochs=30, shuffle = True, verbose = 1, callbacks=[early_stopping])\n",
        "    train_results(history,trial_no)\n",
        "    model.save(\"model_\"+trial_no+\".h5\")\n",
        "    \n",
        "    #run the model on the test data and get test accuracy\n",
        "    pred = model.predict(test, batch_size=(test.samples//test.batch_size+1))\n",
        "    pred_vals = np.argmax(pred, axis=1)\n",
        "    score = accuracy_score(test.classes,pred_vals)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54142674",
      "metadata": {},
      "outputs": [],
      "source": [
        "#hpo - run the trials\n",
        "sampler = TPESampler(seed=123)  # Make the sampler behave in a deterministic way and get reproducable results\n",
        "study = optuna.create_study(direction=\"maximize\",sampler=sampler)\n",
        "study.optimize(hpo, n_trials=25)\n",
        "joblib.dump(study,'./project/results/hyperparameter_optimization/trial_results/study.pkl',)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "best_model = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(best_model.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in best_model.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "    \n",
        "test_results(test,pred_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02fc11f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#get the trials data\n",
        "study_file = open(\"./project/results/hyperparameter_optimization/trial_studies/study.pkl\",\"rb\")\n",
        "final_study = joblib.load(study_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94b0b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "#get trials as csv\n",
        "study_data = pd.DataFrame(final_study.trials_dataframe())\n",
        "data_csv = study_data.to_csv(\"./project/results/hyperparameter_optimization/trial_studies/study.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3d2612",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_study.best_trial.params"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "asl_recognition.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "01b23b1b50af16e23cb2be3e290c408bfda52a4061f35c35206a57bf1fb1b008"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('dl_keras')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
